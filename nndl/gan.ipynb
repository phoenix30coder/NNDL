{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Reshape, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(X_train, _), (_, _) = mnist.load_data()\n",
        "X_train = (X_train.astype(np.float32) - 127.5) / 127.5  # Normalize to [-1, 1]\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)  # Reshape to (N, 28, 28, 1)\n",
        "\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Dense(256, input_dim=100, activation=LeakyReLU(0.2)),\n",
        "        Dense(512, activation=LeakyReLU(0.2)),\n",
        "        Dense(1024, activation=LeakyReLU(0.2)),\n",
        "        Dense(784, activation='tanh'),  # Output an image of size 28x28 (flattened)\n",
        "        Reshape((28, 28))  # Reshape to (28, 28) for the discriminator\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),  # Input shape as (28, 28)\n",
        "        Dense(512, activation=LeakyReLU(0.2)),\n",
        "        Dense(256, activation=LeakyReLU(0.2)),\n",
        "        Dense(1, activation='sigmoid')  # Output a probability\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "gan = Sequential([generator, discriminator])\n",
        "discriminator.trainable = False  # Freeze the discriminator when training the generator\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "def train_gan(epochs, batch_size):\n",
        "    for epoch in range(epochs):\n",
        "        # Train the discriminator\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        real_images = X_train[idx].reshape(-1, 28, 28)  # Reshape back to (28, 28)\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        generated_images = generator.predict(noise)\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(real_images, np.ones((batch_size, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_images, np.zeros((batch_size, 1)))\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "\n",
        "        # Print the progress\n",
        "        print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
        "\n",
        "        # Save images at specific intervals\n",
        "        if epoch % 2 == 0:  # Change save interval as needed\n",
        "            save_images(epoch, generator)\n",
        "\n",
        "def save_images(epoch, generator, examples=25, dim=(5, 5), figsize=(5, 5)):\n",
        "    noise = np.random.normal(0, 1, (examples, 100))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0, 1]\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(examples):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        plt.imshow(generated_images[i, :, :], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"gan_images/mnist_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Create output directory to save generated images\n",
        "os.makedirs('gan_images', exist_ok=True)\n",
        "\n",
        "# Train the GAN for 10 epochs\n",
        "train_gan(epochs=10, batch_size=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AwfJzqKlKa-",
        "outputId": "d1751fe9-1aa6-4725-f11a-f2d93649be0e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7bf6b88f1e10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x7bf6b88f3d00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [D loss: 0.9596980810165405, acc.: 20.70%] [G loss: [array(0.8753231, dtype=float32), array(0.8753231, dtype=float32), array(0.2890625, dtype=float32)]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "1 [D loss: 0.9526920318603516, acc.: 19.14%] [G loss: [array(0.9576504, dtype=float32), array(0.9576504, dtype=float32), array(0.1640625, dtype=float32)]]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "2 [D loss: 1.0227059125900269, acc.: 12.60%] [G loss: [array(1.0522028, dtype=float32), array(1.0522028, dtype=float32), array(0.11458334, dtype=float32)]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "3 [D loss: 1.0951828956604004, acc.: 10.25%] [G loss: [array(1.1361688, dtype=float32), array(1.1361688, dtype=float32), array(0.09570312, dtype=float32)]]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "4 [D loss: 1.19144868850708, acc.: 8.58%] [G loss: [array(1.2418102, dtype=float32), array(1.2418102, dtype=float32), array(0.08125, dtype=float32)]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "5 [D loss: 1.2760279178619385, acc.: 8.03%] [G loss: [array(1.3299209, dtype=float32), array(1.3299209, dtype=float32), array(0.07682291, dtype=float32)]]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "6 [D loss: 1.3709955215454102, acc.: 7.19%] [G loss: [array(1.4293702, dtype=float32), array(1.4293702, dtype=float32), array(0.06919643, dtype=float32)]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "7 [D loss: 1.465303897857666, acc.: 7.27%] [G loss: [array(1.5260737, dtype=float32), array(1.5260737, dtype=float32), array(0.0703125, dtype=float32)]]\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "8 [D loss: 1.564049243927002, acc.: 6.88%] [G loss: [array(1.6264005, dtype=float32), array(1.6264005, dtype=float32), array(0.06684028, dtype=float32)]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "9 [D loss: 1.6525599956512451, acc.: 6.66%] [G loss: [array(1.7090614, dtype=float32), array(1.7090614, dtype=float32), array(0.06484375, dtype=float32)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B-Xazvl8m0FT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}